{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9094b25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.0\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5facaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad752c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader\n",
    "import llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a497f247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 17598 tokens\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader('C:/Users/rojit/Downloads/solr2.txt/data').load_data()\n",
    "index = GPTSimpleVectorIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16eccdc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RetryError",
     "evalue": "RetryError[<Future at 0x2005a1efcd0 state=finished raised AssertionError>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPT\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPT\\lib\\site-packages\\llama_index\\embeddings\\openai.py:142\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(list_of_text, engine)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get embeddings.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03mNOTE: Copied from OpenAI's embedding utils:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_of_text) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe batch size should not be larger than 2048.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# replace newlines, which can negatively affect performance.\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: The batch size should not be larger than 2048.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m doc \u001b[38;5;241m=\u001b[39m llama_index\u001b[38;5;241m.\u001b[39mreaders\u001b[38;5;241m.\u001b[39mJSONReader()\u001b[38;5;241m.\u001b[39mload_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/rojit/Downloads/solr2.txt/data.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m ind \u001b[38;5;241m=\u001b[39m \u001b[43mGPTSimpleVectorIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m ind\u001b[38;5;241m.\u001b[39msave_to_disk(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPT_ind.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPT\\lib\\site-packages\\llama_index\\indices\\vector_store\\vector_indices.py:84\u001b[0m, in \u001b[0;36mGPTSimpleVectorIndex.__init__\u001b[1;34m(self, documents, index_struct, text_qa_template, llm_predictor, embed_model, simple_vector_store_data_dict, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Init params.\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m SimpleVectorStore(\n\u001b[0;32m     81\u001b[0m     simple_vector_store_data_dict\u001b[38;5;241m=\u001b[39msimple_vector_store_data_dict\n\u001b[0;32m     82\u001b[0m )\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     85\u001b[0m     documents\u001b[38;5;241m=\u001b[39mdocuments,\n\u001b[0;32m     86\u001b[0m     index_struct\u001b[38;5;241m=\u001b[39mindex_struct,\n\u001b[0;32m     87\u001b[0m     text_qa_template\u001b[38;5;241m=\u001b[39mtext_qa_template,\n\u001b[0;32m     88\u001b[0m     llm_predictor\u001b[38;5;241m=\u001b[39mllm_predictor,\n\u001b[0;32m     89\u001b[0m     embed_model\u001b[38;5;241m=\u001b[39membed_model,\n\u001b[0;32m     90\u001b[0m     vector_store\u001b[38;5;241m=\u001b[39mvector_store,\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     92\u001b[0m )\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# TODO: Temporary hack to also store embeddings in index_struct\u001b[39;00m\n\u001b[0;32m     95\u001b[0m embedding_dict \u001b[38;5;241m=\u001b[39m vector_store\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39membedding_dict\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPT\\lib\\site-packages\\llama_index\\indices\\vector_store\\base.py:63\u001b[0m, in \u001b[0;36mGPTVectorStoreIndex.__init__\u001b[1;34m(self, documents, index_struct, text_qa_template, llm_predictor, embed_model, vector_store, text_splitter, use_async, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_qa_template \u001b[38;5;241m=\u001b[39m text_qa_template \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_TEXT_QA_PROMPT\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async \u001b[38;5;241m=\u001b[39m use_async\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     64\u001b[0m     documents\u001b[38;5;241m=\u001b[39mdocuments,\n\u001b[0;32m     65\u001b[0m     index_struct\u001b[38;5;241m=\u001b[39mindex_struct,\n\u001b[0;32m     66\u001b[0m     llm_predictor\u001b[38;5;241m=\u001b[39mllm_predictor,\n\u001b[0;32m     67\u001b[0m     embed_model\u001b[38;5;241m=\u001b[39membed_model,\n\u001b[0;32m     68\u001b[0m     text_splitter\u001b[38;5;241m=\u001b[39mtext_splitter,\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     70\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPT\\lib\\site-packages\\llama_index\\indices\\base.py:109\u001b[0m, in \u001b[0;36mBaseGPTIndex.__init__\u001b[1;34m(self, documents, index_struct, llm_predictor, embed_model, docstore, index_registry, prompt_helper, text_splitter, chunk_size_limit, include_extra_info)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_documents(documents)\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# TODO: introduce document store outside __init__ function\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index_from_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# update index registry and docstore with index_struct\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_index_registry_and_docstore()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPT\\lib\\site-packages\\llama_index\\token_counter\\token_counter.py:84\u001b[0m, in \u001b[0;36mllm_token_counter.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_llm_predict\u001b[39m(_self: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m wrapper_logic(_self):\n\u001b[1;32m---> 84\u001b[0m         f_return_val \u001b[38;5;241m=\u001b[39m f(_self, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f_return_val\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPT\\lib\\site-packages\\llama_index\\indices\\base.py:281\u001b[0m, in \u001b[0;36mBaseGPTIndex.build_index_from_documents\u001b[1;34m(self, documents)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;129m@llm_token_counter\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_index_from_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_index_from_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, documents: Sequence[BaseDocument]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m IS:\n\u001b[0;32m    280\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build the index from documents.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_index_from_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPT\\lib\\site-packages\\llama_index\\indices\\vector_store\\base.py:206\u001b[0m, in \u001b[0;36mGPTVectorStoreIndex._build_index_from_documents\u001b[1;34m(self, documents)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[1;32m--> 206\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_document_to_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPT\\lib\\site-packages\\llama_index\\indices\\vector_store\\base.py:182\u001b[0m, in \u001b[0;36mGPTVectorStoreIndex._add_document_to_index\u001b[1;34m(self, index_struct, document)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add document to index.\"\"\"\u001b[39;00m\n\u001b[0;32m    181\u001b[0m nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_nodes_from_document(document)\n\u001b[1;32m--> 182\u001b[0m embedding_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node_embedding_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_doc_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m new_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39madd(embedding_results)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# if the vector store doesn't store text, we need to add the nodes to the\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# index struct\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPT\\lib\\site-packages\\llama_index\\indices\\vector_store\\base.py:102\u001b[0m, in \u001b[0;36mGPTVectorStoreIndex._get_node_embedding_results\u001b[1;34m(self, nodes, existing_node_ids, doc_id)\u001b[0m\n\u001b[0;32m     99\u001b[0m     id_to_node_map[new_id] \u001b[38;5;241m=\u001b[39m n\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# call embedding model to get embeddings\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m result_ids, result_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_queued_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m new_id, text_embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result_ids, result_embeddings):\n\u001b[0;32m    104\u001b[0m     id_to_embed_map[new_id] \u001b[38;5;241m=\u001b[39m text_embedding\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPT\\lib\\site-packages\\llama_index\\embeddings\\base.py:151\u001b[0m, in \u001b[0;36mBaseEmbedding.get_queued_text_embeddings\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m cur_batch_ids \u001b[38;5;241m=\u001b[39m [text_id \u001b[38;5;28;01mfor\u001b[39;00m text_id, _ \u001b[38;5;129;01min\u001b[39;00m cur_batch]\n\u001b[0;32m    150\u001b[0m cur_batch_texts \u001b[38;5;241m=\u001b[39m [text \u001b[38;5;28;01mfor\u001b[39;00m _, text \u001b[38;5;129;01min\u001b[39;00m cur_batch]\n\u001b[1;32m--> 151\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_batch_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m result_ids\u001b[38;5;241m.\u001b[39mextend(cur_batch_ids)\n\u001b[0;32m    153\u001b[0m result_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPT\\lib\\site-packages\\llama_index\\embeddings\\openai.py:260\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_text_embeddings\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid mode, model combination: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    259\u001b[0m     engine \u001b[38;5;241m=\u001b[39m _TEXT_MODE_MODEL_DICT[key]\n\u001b[1;32m--> 260\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPT\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPT\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPT\\lib\\site-packages\\tenacity\\__init__.py:326\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m retry_exc\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait:\n\u001b[0;32m    329\u001b[0m     sleep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(retry_state)\n",
      "\u001b[1;31mRetryError\u001b[0m: RetryError[<Future at 0x2005a1efcd0 state=finished raised AssertionError>]"
     ]
    }
   ],
   "source": [
    "doc = llama_index.readers.JSONReader().load_data(\"C:/Users/rojit/Downloads/solr2.txt/data.json\")\n",
    "ind = GPTSimpleVectorIndex(doc)\n",
    "ind.save_to_disk('GPT_ind.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcce4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index.save_to_disk('index.json')\n",
    "# load from disk\n",
    "index = GPTSimpleVectorIndex.load_from_disk('index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7954af5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 4003 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The author grew up writing short stories, programming on an IBM 1401, and working on microcomputers. He wrote simple games, a program to predict how high his model rockets would fly, and a word processor. He studied philosophy in college, but switched to AI. He reverse-engineered SHRDLU for his undergraduate thesis and wrote a book about Lisp hacking. He visited the Carnegie Institute and realized he could make art that would last. He took art classes at Harvard and applied to RISD and the Accademia di Belli Arti in Florence. He also arrived at an arrangement whereby the students wouldn't require the faculty to teach anything, and in return the faculty wouldn't require the students to learn anything. He took advantage of this arrangement to paint a nude model who lived down the street from him, while the other students chatted or tried to imitate things they'd seen in American art magazines.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30d347a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 54464 tokens\n"
     ]
    }
   ],
   "source": [
    "doc = SimpleDirectoryReader('C:/Users/rojit/Downloads/GPT_data-eng/data').load_data()\n",
    "ind = GPTSimpleVectorIndex(doc)\n",
    "ind.save_to_disk('C:/Users/rojit/Downloads/GPT_data-eng/data/GPT_ind.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c3ff060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n",
      "INFO:root:> [query] Total LLM token usage: 3941 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 6 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KrisFlyer is a global frequent flyer programme run by Singapore Airlines. It allows members to earn KrisFlyer miles through both flight and non-flight related activities, which can be used for the redemption of award flights and upgrades, among many other options. Elite miles, on the other hand, can only be earned when flying with Singapore Airlines and selected partner airlines. These miles count towards Elite Silver and Elite Gold tier qualification, but are not redeemable for award flights, upgrade awards or purchase of airfares. For Elite Gold members, Elite miles can be used to redeem for additional benefits.\n"
     ]
    }
   ],
   "source": [
    "ind = GPTSimpleVectorIndex.load_from_disk('C:/Users/rojit/Downloads/GPT_data-eng/data/GPT_ind.json')\n",
    "response = ind.query(\"what is KrisFlyer?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1d79c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3740 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 5 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ranald is not mentioned in the context information provided.\n"
     ]
    }
   ],
   "source": [
    "response = ind.query(\"what is Ranald?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a0a6f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n",
      "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: HTTP code 502 from API (<html>\n",
      "<head><title>502 Bad Gateway</title></head>\n",
      "<body>\n",
      "<center><h1>502 Bad Gateway</h1></center>\n",
      "<hr><center>nginx</center>\n",
      "</body>\n",
      "</html>\n",
      ").\n",
      "INFO:root:> [query] Total LLM token usage: 4099 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 7 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "To do online check in, go to the SingaporeAir website or mobile app and enter your Booking Reference number. You will then be guided through a few easy steps to check in for your flight. You may also be asked to enter your passport information for security reasons. Once you arrive at the confirmation page, your check-in is complete. Depending on your city of departure, you may be eligible for digital or mobile boarding passes. If so, these boarding passes will be available for download in PDF format, on your SingaporeAir mobile app, or printed at the airport check-in counter or kiosk (where available). Generating your boarding pass or downloading it to your mobile device before you go to the airport will help you avoid the queue at the check-in counter. If you have bags to check in or travel documents to verify, you can proceed to the bag drop counters.\n"
     ]
    }
   ],
   "source": [
    "response = ind.query(\"How to do online check in?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d543e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3942 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 5 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Astronomy is the scientific study of celestial objects, such as stars, planets, comets, nebulae, and galaxies, and phenomena that originate outside the Earth's atmosphere. It is concerned with the evolution, physics, chemistry, meteorology, and motion of celestial objects, as well as the formation and development of the universe. It does not involve opting out of the unaccompanied minor service.\n"
     ]
    }
   ],
   "source": [
    "response = ind.query(\"what is Astronomy?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1404ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3657 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 4 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Astronomy is not mentioned in the context information.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"what is astronomy?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef9e542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 41550 tokens\n"
     ]
    }
   ],
   "source": [
    "docu = SimpleDirectoryReader('C:/Users/rojit/Downloads/GPT_data-eng/data').load_data()\n",
    "inde = GPTSimpleVectorIndex(docu)\n",
    "inde.save_to_disk('C:/Users/rojit/Downloads/GPT_data-eng/data/GPT_ind_new.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e97a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inde = GPTSimpleVectorIndex.load_from_disk('C:/Users/rojit/Downloads/GPT_data-eng/code/GPT_ind_new.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dd0c07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3738 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 6 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KrisFlyer is a global frequent flyer programme offered by Singapore Airlines. It allows members to earn miles through both flight and non-flight related activities, which can be used for the redemption of award flights and upgrades, among many other options. All credit / debit card transactions are processed securely through our fraud detection system and bank authentication programmes, which are fully compliant with international PCI DSS security standards. Payment methods cannot be changed once your payment has been processed, and all transactions are subject to the approval of your issuing bank / payment provider.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = inde.query(\"what is KrisFlyer?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c68eebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 2379 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 5 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Farming is a complex process that requires knowledge and experience. It involves a variety of activities such as soil preparation, planting, harvesting, and marketing. To learn more about farming, you can visit your local agricultural extension office or research online. You can also find resources on how to start a farm, what equipment you need, and how to manage your farm.\n"
     ]
    }
   ],
   "source": [
    "response = inde.query(\"How to do farming?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ad2d1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3825 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 9 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Living like a Spartan involves living a life of simplicity and self-discipline. This includes eating a healthy diet, exercising regularly, and avoiding material possessions and luxuries. It also involves living with a sense of purpose and dedication to a cause, such as a career or a cause that you believe in. Additionally, it involves living with integrity and honesty, and being mindful of your actions and their consequences. To ensure financial security, it is important to use secure payment methods such as credit/debit cards with fraud detection systems and bank authentication programmes, and to be mindful of the payment methods you use to make purchases.\n"
     ]
    }
   ],
   "source": [
    "response = inde.query(\"how to live like a spartan?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc83ee18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 32430 tokens\n"
     ]
    }
   ],
   "source": [
    "d = SimpleDirectoryReader('C:/Users/rojit/Downloads/GPT_data-eng/data').load_data()\n",
    "i = GPTSimpleVectorIndex(d)\n",
    "i.save_to_disk('C:/Users/rojit/Downloads/GPT_data-eng/code/GPT_ind_new_2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bbc9820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = GPTSimpleVectorIndex.load_from_disk('C:/Users/rojit/Downloads/GPT_data-eng/code/GPT_ind_new_2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfc9b6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3844 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 6 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "KrisFlyer is a global frequent flyer programme offered by Singapore Airlines. It is free to join and members can earn KrisFlyer miles through both flight and non-flight related activities. These miles can be used for the redemption of award flights and upgrades, among many other options. Elite miles can only be earned when flying with Singapore Airlines and selected partner airlines. Members can also access exclusive benefits such as the ability to cancel waitlisted redemptions online, ticket reserved segments online, and receive periodic email reminders on waitlisted redemptions.\n"
     ]
    }
   ],
   "source": [
    "response = i.query(\"what is KrisFlyer?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1460f9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3621 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The types of boarding passes available are kiosk boarding pass, mobile boarding pass, digital boarding pass, and HighFlyer boarding pass. HighFlyer boarding passes are available to registered businesses of all sizes and consist of four tiers: HighFlyer, HighFlyer Silver, HighFlyer Gold, and HighFlyer Platinum.\n"
     ]
    }
   ],
   "source": [
    "response = i.query(\"What types of boarding passes are available?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4c10303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3556 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 9 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It is not possible to answer this question given the context information provided.\n"
     ]
    }
   ],
   "source": [
    "response = i.query(\"how to live like a spartan?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7e7f1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3539 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 5 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Farming is not related to the context information provided.\n"
     ]
    }
   ],
   "source": [
    "response = i.query(\"How to do farming?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "20f232ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 4276 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 7 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Once upon a time, there was a family of four who loved to travel. They had been planning a trip to Europe for months and were so excited to finally be able to go. \n",
      "\n",
      "The family arrived at the airport and checked in for their flight. They were amazed at how easy it was to check in online and watch the video tutorial to learn how to check in as a group. They were also thankful for the airport police officer who guided them to scan their boarding passes at the PRS before proceeding to immigration. \n",
      "\n",
      "The family was also thankful for the ESTA website which enabled them to travel to the USA as long as they were citizens of an eligible country under the Visa Waiver Program (VWP). They also had to make sure they had the necessary visas and passports before they were allowed to travel to Australia and/or New Zealand. \n",
      "\n",
      "The family was also thankful for the Singapore Airlines mobile app which allowed them to access their digital KrisFlyer/PPS Club membership card anytime, anywhere. This enabled them to enjoy the applicable membership privileges and benefits when they travelled on Singapore Airlines or transacted with their KrisFlyer airline and non-airline partners. The family was also thankful for the KrisFlyer Miles they had earned\n"
     ]
    }
   ],
   "source": [
    "response = i.query(\"write a story regarding air planes?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758f78a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
